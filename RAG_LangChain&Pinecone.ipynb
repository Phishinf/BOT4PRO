{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAanX3lkIyJAD4+ClSrWXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phishinf/BOT4PRO/blob/main/RAG_LangChain%26Pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval Augmentation in LangChain\n",
        "\n",
        "LLMs have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
        "\n",
        "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
        "\n",
        "This example covers the steps to integrate Pinecone, a high-performance vector database, with LangChain, a framework for building applications powered by large language models (LLMs).\n",
        "\n",
        "Pinecone enables developers to build scalable, real-time recommendation and search systems based on vector similarity search. LangChain, on the other hand, provides modules for managing and optimizing the use of language models in applications"
      ],
      "metadata": {
        "id": "l3K65Qus8wQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Environmnt Packages"
      ],
      "metadata": {
        "id": "9J46NbPp_h_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkEsbPfU8j5N"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain==0.0.162\n",
        "!pip install openai tiktoken \"pinecone-client[grpc]\" datasets\n",
        "!pip install apache_beam mwparserfromhell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install multiprocess==0.70.15"
      ],
      "metadata": {
        "id": "SswB4VIHD6sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Knowledge Base\n",
        "Every record contains a lot of text. It is useful chunking these articles/documents into more \"concise\" chunks to later be embedding and stored in our Pinecone vector database.\n",
        "\n",
        "By using LangChain's RecursiveCharacterTextSplitter to split our text into chunks of a specified max length."
      ],
      "metadata": {
        "id": "-3lUPmKMArcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
        "data\n",
        "data[6]\n",
        "# Use any LLM to do\n",
        "import tiktoken\n",
        "tiktoken.encoding_for_model('gpt-3.5-turbo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNZsWxer_gMg",
        "outputId": "c5311d29-6498-4b06-b714-88cb9b89ba64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Encoding 'cl100k_base'>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the abovee Encoding size and fill in \"size\" below\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
        "             \"we can find the length of this chunk of text in tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EePCtzz6ggC",
        "outputId": "e955bbe7-3f97-4455-8e66-55efcc8f7801"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Divide the text into chunks"
      ],
      "metadata": {
        "id": "OJLUt1BH6hCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Relation between chunk size and\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "# Using the text_splitter get much better sized chunks of text.\n",
        "# Use this functionality during the indexing process too.\n",
        "chunks = text_splitter.split_text(data[6]['text'])[:4]\n",
        "chunks\n",
        "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sk5TYCk7sSR",
        "outputId": "5c9008a2-69f5-4af7-d72c-ad4f4f0fc4e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470, 469, 222)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Embeddings\n",
        "Building embeddings using LangChain's OpenAI or other LLM embedding. Usually first it needa to add OpenAI api key by running the next cell:"
      ],
      "metadata": {
        "id": "EsDKEeUI-PEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")  # platform.openai.com\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = 'text-embedding-ada-002'\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "texts = [\n",
        "    'this is the first chunk of text',\n",
        "    'then another second chunk of text is here'\n",
        "]\n",
        "\n",
        "res = embed.embed_documents(texts)\n",
        "len(res), len(res[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "_OpVOqsZ-k4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector Database\n",
        "To create our vector database need a free API key from Pinecone.\n",
        "\n",
        "Initialize like so:"
      ],
      "metadata": {
        "id": "zUVtUgBp_JYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "YOUR_API_KEY = getpass(\"Pinecone API Key: \")\n",
        "# find ENV (cloud region) next to API key in console\n",
        "YOUR_ENV = input(\"Pinecone environment: \")\n",
        "\n",
        "index_name = 'langchain-retrieval-augmentation'\n",
        "pinecone.init(\n",
        "    api_key=YOUR_API_KEY,\n",
        "    environment=YOUR_ENV\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
        "    )\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "\n",
        "index.describe_index_stats()\n",
        "#This Pinecone index has a total_vector_count of 0, as haven't added any vectors yet.\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "batch_limit = 100\n",
        "\n",
        "texts = []\n",
        "metadatas = []\n",
        "\n",
        "for i, record in enumerate(tqdm(data)):\n",
        "    # first get metadata fields for this record\n",
        "    metadata = {\n",
        "        'wiki-id': str(record['id']),\n",
        "        'source': record['url'],\n",
        "        'title': record['title']\n",
        "    }\n",
        "    # now we create chunks from the record text\n",
        "    record_texts = text_splitter.split_text(record['text'])\n",
        "    # create individual metadata dicts for each chunk\n",
        "    record_metadatas = [{\n",
        "        \"chunk\": j, \"text\": text, **metadata\n",
        "    } for j, text in enumerate(record_texts)]\n",
        "    # append these to current batches\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    # if we have reached the batch_limit we can add texts\n",
        "    if len(texts) >= batch_limit:\n",
        "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "        embeds = embed.embed_documents(texts)\n",
        "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "        texts = []\n",
        "        metadatas = []\n",
        "\n",
        "if len(texts) > 0:\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    embeds = embed.embed_documents(texts)\n",
        "    index.upsert(vectors=zip(ids, embeds, metadatas))\n"
      ],
      "metadata": {
        "id": "eK-5LIwz_hNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check the number of vector in the Index\n",
        "\n",
        "Initializing a vector store using the same index we just built."
      ],
      "metadata": {
        "id": "quQnghc1PlMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")\n",
        "# An example\n",
        "query = \"who was Benito Mussolini?\"\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ],
      "metadata": {
        "id": "mh75KNWNPpmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generative Question-Answering\" or GQA\n",
        "\n",
        "In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the vectorstore.\n",
        "\n",
        "To do this we initialize a RetrievalQA object like so:"
      ],
      "metadata": {
        "id": "9D-kgE_TQT2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "qa.run(query)"
      ],
      "metadata": {
        "id": "QiEhPQRkQqS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RQA, the sources of information that the LLM using to answer our question\n",
        "\n",
        " Using a slightly different version of RetrievalQA called RetrievalQA With SourcesChain:"
      ],
      "metadata": {
        "id": "CPIsmlXzQ7D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "qa.with_sources(query)\n"
      ],
      "metadata": {
        "id": "0sPJSZ4JRdJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Answer the question being asked, and return the source of this information being used by the LLM."
      ],
      "metadata": {
        "id": "_x4OsB_NTqr3"
      }
    }
  ]
}